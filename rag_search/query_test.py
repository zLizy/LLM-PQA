# 主系统，执行后在gr中输入query
import pymongo

from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_community.document_loaders.directory import DirectoryLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_community.llms import openai

import os
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
from gradio.themes.base import Base


import numpy as np




import pandas as pd


from query_manager import QueryManager

import key_param  

import re

# import logging

# # 配置日志
# logging.basicConfig(filename='system_logs.log', level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')

# def query_model(query):
#     logging.debug("Starting model query")
#     # 你的代码逻辑
#     logging.debug("Model query completed")

# # 其他函数中也可以加入类似的日志记录


load_dotenv()

AIclient = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY")
)

client = pymongo.MongoClient("mongodb+srv://arslan:771944972@cluster0.qv2ymat.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0")
model_collection = client["models"]["collection_of_model_description"]
database_collection = client["database"]["collection_of_database_description"]

embeddings = OpenAIEmbeddings(openai_api_key=key_param.OPENAI_API_KEY)

vectorStore = MongoDBAtlasVectorSearch(model_collection, embeddings)

vectorStore_for_database = MongoDBAtlasVectorSearch(database_collection, embeddings)

def query_model(query):
    docs = vectorStore.similarity_search(query,K=3)  #perform an atlas vector search using lang chain's vector store, retrieve the most similar document based on the query vector
    # as_output = docs[0].page_content #extract the page content from the top document in the list(most relevant info)
    as_output = '匹配的第一个modelprofile是：' + docs[0].page_content + '\n\n匹配的第二个modelprofile是：' + docs[1].page_content + '\n\n匹配的第三个modelprofile是：' + docs[2].page_content  
    # llm = OpenAI(openai_api_key=key_param.OPENAI_API_KEY, temperature = 0) 
    # llm = ChatOpenAI(openai_api_key=key_param.OPENAI_API_KEY)
    # retriever = vectorStore.as_retriever()                                     #use LLM to fetch documents that are relevant to the query
    # qa = RetrievalQA.from_chain_type(llm,chain_type="stuff",retriever=retriever)
    # retriever_output = qa.invoke(query)
    return as_output 

    # return as_output, retriever_output   #as_output is the most similar document from the atlas vector search, retriever_output is generated by RAG

def query_database(query):
    # 利用RAG和vector search技术进行数据库的匹配查询
    docs = vectorStore_for_database.similarity_search(query, K=1)  # 检索最相似的数据库描述文档
    as_output = docs[0].page_content #extract the page content from the top document in the list(most relevant info)
    # llm = OpenAI(openai_api_key=key_param.OPENAI_API_KEY, temperature = 0) 
    # llm = ChatOpenAI(openai_api_key=key_param.OPENAI_API_KEY)
    # retriever = vectorStore_for_database.as_retriever()                                     #use LLM to fetch documents that are relevant to the query
    # qa = RetrievalQA.from_chain_type(llm,chain_type="stuff",retriever=retriever)
    # retriever_output = qa.invoke(query)
    return as_output
    # return as_output, retriever_output

def simplify_model_description(description):
    """
    从模型描述中提取模型名称。
    """
    # 使用正则表达式寻找 "Model Name:" 后紧接的任何非换行字符，直到遇到换行符
    # match = re.search(r"Model Name:\s*(\S+)", description)
    # match = re.search(r"Model Name:\s*\**([^*\n]+)\**", description)
    match = re.search(r"Model Name:\s*\**\s*([^*\n]+)\**", description)
    if match:
        return match.group(1).strip()  # 返回匹配到的第一个组，即模型名称
    return "Unknown"

def exact_dataset_from_model_profile(description):
    dataset_match = re.search(r"Dataset Name:\s*\**\s*([^*\n]+)\**", description)
    # dataset_match = re.search(r"Dataset Name:\s*(\S+)", description)
    if dataset_match:
        return dataset_match.group(1).strip()
    return "Unknown"

def extract_accuracy(model_profile):
    match = re.search(r"Accuracy:\s*([\d.]+)", model_profile)
    return float(match.group(1)) if match else 0.0

# def simplify_database_decision(description):
#     """
#     根据描述内容决定使用哪个数据库。
#     """

#     # 定义一组可能的数据库描述关键词
#     database_keywords = {
#         "transactions": ["financial transactions", "account", "banking", "transaction"],
#         "weather": ["weather condition", "weather", "precipitation", "temp_max", "temp_min", "wind"],
#         "spotify":["music","playlist"]
#     }

#     # 遍历关键词字典，查找首个在描述中出现的数据库关键词组
#     for database, keywords in database_keywords.items():
#         if any(keyword in description.lower() for keyword in keywords):

#             return database  # 返回匹配的数据库名称

#     return "unknown"  # 如果没有找到匹配，返回"unknown"

def simplify_dataset_description(description):
    """
    从数据库描述中提取数据名称。
    """

    # match = re.search(r"Dataset Name:\s*\**([^*\n]+)\**", description)
    match = re.search(r"Dataset (Name|Profile):\s*\**\s*([^*\n]+)\**", description)
    if match:
        return match.group(2).strip()  # 返回匹配到的第一个组，即数据名称
    return "Unknown"

# def detect_intent_with_openai(query):
#     prompt = (
#         f"请根据以下用户查询内容判断意图，并给出回答。如果用户查询的内容可以从数据库中直接提取，请回答'1'，如果用户查询的意图无法从数据库中直接获取，请回答'0'。下面是几个具体例子："
#         f"如果查询的意图是预测用户交易金额，例如<I want to know the user 51617's next transaction amount>，请回答'0'。"
#         f"如果查询的意图是预测天气状况，例如<predict weather condition>，也请回答'0'。"
#         f"如果查询的意图是艺术家名，例如<Coldplay>，也请回答'0'。"
#         f"如果查询的意图是数据库中已有的气温，例如<2012-01-01 temperature或者I'm looking for the temperature on 20210101>，请回答'1'。"
#         f"如果查询的意图是用户过去的交易金额，例如<I want to know the user 51617's previous transaction amount>，请回答'1'。"
#         f"否则，请回答'1'。\n\n用户查询：'{query}'\n\n回答："
#     )
#     response = AIclient.chat.completions.create(
#         model="gpt-3.5-turbo",
#         messages=[
#             {"role": "system", "content": prompt},
#             {"role": "user", "content": query}
#         ]
#     )
#     answer = response.choices[0].message.content.strip()

#     return answer

def get_dataset_files(directory):
    return [file for file in os.listdir(directory) if file.endswith('.csv')]

dataset_files = get_dataset_files('D:\\Program Files\\Code repositories\\RAG\\RAG\\rag_search\\database_files')
model_options = ['RegressionModel', 'Recommender'] 

def beautify_response(response_message):

    prompt = f"""
    The following response message needs to be beautified for display in a Streamlit chatbot. Specifically, input features should be presented separately, preferably in a table format. Additionally, improve the overall readability and presentation.

    Example 2: If the response_message contains "which is different from the dataset 'Unknown' we matched.", modify it to "We couldn't find a suitable dataset. Please modify the query content based on the dataset information."

    Formatting requirements:
    1."We found the model... Both the dataset and model are aligned." should be presented in the beginning.
 
    2. Model Overview should be presented as a subheading (e.g., Heading 2).
    3. In the Model Overview section, create a table with two columns, please do not use <br> tags as they are not recognized.:
    - First column: Titles (Model Name, Dataset, Input Features).
    - Second column: Corresponding values (Model name, dataset name, input features)
    4. Model Performance should be presented another subheading (e.g., Heading 2), followed by the relevant information. 

    Here's the response message that needs beautification:
    
    {response_message}
    """

    #    2.If the response message contains "We found the model **'Unknown'** trained with the dataset **'Unknown'**. Both the dataset and model are aligned." drop all other message and only reply with "We couldn't find a suitable model. You can respond with 'new' to train a new model.", also do not provide with any table.

    response = AIclient.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": response_message}
        ]
    )

    new_response = response.choices[0].message.content.strip()
    return new_response

def initial_query(query):

    # _, retriever_output = query_model(query)
    # model_info, _ = query_model(query)
    model_info = query_model(query)

    model_description = model_info if isinstance(model_info, str) else model_info.get('result', '')    
    print('RAG提取的model profile是：', model_description)

    profiles = model_description.split('匹配的第')

    profiles = [p for p in profiles if p.strip()]

    models = []

    for profile in profiles:
        model_name = simplify_model_description(profile)
        dataset_name = exact_dataset_from_model_profile(profile)
        accuracy = extract_accuracy(profile)
        
        models.append({
            'name': model_name,
            'dataset': dataset_name,
            'accuracy': accuracy,
            'profile': profile
        })

    best_model = max(models, key=lambda x: x['accuracy'])

    best_model_description = best_model['profile']



    # model_overview_match = re.search(r"Model Overview:\s*(.+?)(?=\n|$)", model_description)
    # input_features_match = re.search(r"Input Features:\s*\[(.*?)\]", model_description)
    # output_match = re.search(r"Output:\s*(.+?)(?=\n|$)", model_description)
    # model_performance_match = re.search(r"Model Performance:\s*(.+?)(?=\n|$)", model_description)
    model_overview_match = re.search(r"Model Overview:\s*(.+?)(?=\n\n|$)", best_model_description, re.DOTALL)
    # input_features_match = re.search(r"Input Features:\s*(.+?)(?=Output:|\n\n|$)", model_description, re.DOTALL)
    # output_match = re.search(r"Output:\s*(.+?)(?=\n\n|$)", model_description, re.DOTALL)
    model_performance_match = re.search(r"Model Performance:\s*(.+?)(?=\n\n|$)", best_model_description, re.DOTALL)

    model_overview = model_overview_match.group(1) if model_overview_match else "Model overview not available."
    # input_features = input_features_match.group(1).strip() if input_features_match else "Input features not specified."
    # output = output_match.group(1).strip() if output_match else "Output details not available."
    model_performance = model_performance_match.group(1).strip() if model_performance_match else "Model performance details not available."

    model_name = best_model['name']
    model_dataset_name = best_model['dataset']
    
    print("Model Name:", model_name)
    print("Model Dataset Name:", model_dataset_name) 





    # model_name = simplify_model_description(model_description)
    # print("Model Name:", model_name)

    # model_dataset_name = exact_dataset_from_model_profile(model_description)
    # print("Model Dataset Name:", model_dataset_name) 




    

    # dataset_info, _ = query_database(query)
    dataset_info= query_database(query)

    dataset_description = dataset_info if isinstance(dataset_info, str) else dataset_info.get('result', '')
    # print('RAG提取的database profile是：', dataset_description)

    # dataset_overview_match = re.search(r"Overview:\s*(.+?)(?=\n|$)", dataset_description)
    # dataset_overview_match = re.search(r"Overview:\s*(.+?)(?=\n\n|$)", dataset_description, re.DOTALL)
    # dataset_overview = dataset_overview_match.group(1) if dataset_overview_match else "Dataset overview not available."

    dataset_name = simplify_dataset_description(dataset_description)
    print("Dataset Name:", dataset_name)
    print("匹配到的Dataset Profile:", dataset_description)
    
    model_details = f"**Model Overview:**\n{model_overview}\n\n" \
                    f"**Model Performance:**\n{model_performance}\n\n"

    # datafile_name = dataset_name +  ".csv"
    # selected_file_path = os.path.join('D:\\Program Files\\Code repositories\\RAG\\RAG\\rag_search\\database_files', datafile_name)
    # df = pd.read_csv(selected_file_path, on_bad_lines='skip')
    # sample_data_markdown = df.head().to_markdown(index=False)

    action_prompt = f"We found the model **'{model_name}'** trained with the dataset **'{model_dataset_name}'**. " \
                    f"Both the dataset and model are aligned. "
                    

    if model_dataset_name == dataset_name:
        # response =  f"{action_prompt}\n\n{model_details}\n\n The first few rows of the matched dataset **'{dataset_name}'** are:\n\n{sample_data_markdown}\n\n"
        response =  f"{action_prompt}\n\n{model_details}\n\n" \
                    f"Would you like to proceed with this model and dataset? " \
                    f"**Respond with 'yes' to proceed or 'new' to select a new model and dataset**."
    else:
        response = f"Based on your query information, we matched the model **'{model_name}'**, but the dataset used to train this model is **'{model_dataset_name}'**," \
                   f"which is different from the dataset **'{dataset_name}'** we matched. You can respond with 'new' to train a new model."

    # # 对比模型和数据集名称
    # if model_dataset_name == dataset_name:
    #     response = f"Found model: {model_name} trained by dataset: {model_dataset_name}. Existing dataset found: {dataset_name}. Model and dataset match. If you wish to use this model and data, please response with 'y', if you want to train new model, please response with 'new'"
    # else:
    #     response = f"Found model: {model_name} trained by dataset: {model_dataset_name}. Existing dataset found: {dataset_name}. Model and data do not match. Please response with 'new'"
    print('LLM收到的raw message是：', response)
    beautified_response = beautify_response(response)

    # demo = "We matched a dataset **Student_Performance.csv**, but we couldn't find a suitable model. You can respond with 'new' to train a new model with the dataset."
    # dataset = 'Student_Performance'
    # return demo, {'model_name': model_name, 'dataset_name': dataset}
    return beautified_response, {'model_name': model_name, 'dataset_name': dataset_name}

def finalize_decision(query, user_response, state):
    if user_response.lower() == 'y':
        query_manager = QueryManager()
        handler = query_manager.get_handler(state['model_name'], None, state['dataset_name'])
        model_name, result, accuracy_info = handler.handle_query(query, state['model_name'], state['dataset_name'], use_new=False)

        raw_response = f"The prediction result is: **{result}**. " 
        
                    #  f"The prediction result using the model **'{model_name}'** and dataset **'{state['dataset_name']}'** is: **{result}**. " \
                    # f"The model's accuracy metrics are as follows: **{accuracy_info}** " 
        
        response = beautify_final_response(raw_response)

        # response = f"Result: {result}, Accuracy: {accuracy_info}, Using matched model {model_name} and dataset {state['dataset_name']}"
    elif user_response.lower() == 'new':
        raw_response = "Available machine learning models include: 'RegressionModel', 'Recommender'"

        response = beautify_final_response(raw_response)
    
    return response

def handle_new_model_selection(selected_model, selected_dataset, query):
    query_manager = QueryManager()
    selected_dataset = selected_dataset[:-4]
    print('选择的数据集名是:', selected_dataset)
    handler = query_manager.get_handler(selected_model, None, selected_dataset)  # 去掉最后的 '.csv'
    model_name, result, accuracy_info = handler.handle_query(query, selected_model, selected_dataset, use_new=True)

    # response = f"The prediction result using a new model **'{model_name}'** trained on the dataset **'{selected_dataset}'** is: **{result}**. " \
    #             f"The model's accuracy metrics are as follows: **{accuracy_info}** " 
    
    raw_response = f"\n\nFor your query: \"{query}\", the results are as follows:\n\n" \
               f"**{result}**\n\n" \
               f"We have trained a new model named **'{model_name}'**. The performance for this model are **{accuracy_info}**.\n\n"
    
    response = beautify_final_response(raw_response)

    # response = f"New model {model_name} trained on dataset {selected_dataset}: Result - {result}, Accuracy - {accuracy_info}"
    return response


def beautify_final_response(response_message):
    prompt = f"""
    Beautify the following response message for display in a Streamlit chatbot. Improve the overall readability, presentation, and professionalism:

    1. Highlight the result values more prominently compared to other information.
    2. do not add any heading!
    3. Ensure the response message is well-structured and visually appealing.

    Example:
    - For a response containing "The prediction result using the model and dataset is: (result).", ensure the result is prominently highlighted and displayed on a separate line.
    - For a response containing "For your query: the results are as follows: (result)", ensure the result is prominently highlighted and displayed on a separate line.

    Response message:
    
    {response_message}
    """

    response = AIclient.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": response_message}
        ]
    )

    new_response = response.choices[0].message.content.strip()
    return new_response


# def initial_query(query, state):

#     if state is None:
#         state = {}

#     # _, retriever_output = query_model(query)
#     as_output, _ = query_model(query)
#     model_description = as_output if isinstance(as_output, str) else as_output.get('result', '')

#     # model_description = retriever_output if isinstance(retriever_output, str) else retriever_output.get('result', '')
        
#     print('RAG提取的model profile是：', model_description)
#     model_name = simplify_model_description(model_description)
#     print("Model Name:", model_name)

#     model_dataset_name = exact_dataset_from_model_profile(model_description)
#     print("Model Dataset Name:", model_dataset_name) 

#     as_output, _ = query_database(query)
#     dataset_description = as_output if isinstance(as_output, str) else as_output.get('result', '')
#     # print('RAG提取的database profile是：', dataset_description)

#     dataset_name = simplify_dataset_description(dataset_description)
#     print("Dataset Name:", dataset_name)

#     state.update({
#         'model_name': model_name,
#         'model_dataset_name': model_dataset_name,
#         'dataset_name': dataset_name,
#         'user_decision': None  # 这将用来存储用户的决策
#     })
#     # confirmation_message = f"Found model: {model_name} trained by dataset: {model_dataset_name}. "
#     # confirmation_message += f"Exsiting dataset found: {dataset_name}."

#     if model_dataset_name == dataset_name:
#         confirmation_message = (f"Found model: {model_name} trained by dataset: {model_dataset_name}. Existing dataset found: {dataset_name}. "
#                                 "Model and dataset match. If you wish to use this model and data, "
#                                 "please check 'Matched' and click 'Confirm' button.")
#     else:
#         confirmation_message = (f"Found model: {model_name} trained by dataset: {model_dataset_name}. Existing dataset found: {dataset_name}. "
#                                 "Model and data do not match. Please check 'Train New Model', select your desired model and dataset "
#                                 "from the dropdowns and click 'Confirm' button to train a new model.")


#     return confirmation_message, state

# def finalize_decision(query, confirm_use, selected_model, selected_dataset, state):
#     if confirm_use:
#         query_manager = QueryManager()  
#         handler = query_manager.get_handler(state['model_name'], None, state['dataset_name'])
#         model_name, result, accuracy_info = handler.handle_query(query, state['model_name'], state['dataset_name'], use_new=False)
#         return result, accuracy_info, f"Using matched model {model_name} and dataset {state['dataset_name']}"
#     else:
#         query_manager = QueryManager()
#         selected_dataset = selected_dataset[:-4] # 去掉最后的 '.csv'
#         handler = query_manager.get_handler(selected_model, None, selected_dataset)
#         model_name, result, accuracy_info = handler.handle_query(query, selected_model, selected_dataset, use_new=True)
#         state.clear()
#         return result, accuracy_info, f"New model {model_name} trained on dataset {selected_dataset}"


# with gr.Blocks(theme=Base(),title="Question Answering App") as app:
#     gr.Markdown(
#         """
#         #Question Answering App
#         """)
#     query_input = gr.Textbox(label="Enter your query")
#     with gr.Row():
#         submit_query_btn = gr.Button("Submit Query",variant="primary")

#     confirm_use = gr.Checkbox(label="Matched", value=False)
#     use_new = gr.Checkbox(label="Train new model", value=False)

#     selected_model = gr.Dropdown(label="Select a dataset", choices=model_options)
#     selected_dataset = gr.Dropdown(label="Select a model", choices=dataset_files) 

#     with gr.Row():
#         Confirm_btn = gr.Button("Confirm",variant="primary")

#     confirmation_text = gr.Textbox(label="User Guide")
#     result_text = gr.Textbox(label="Result")
#     accuracy_text = gr.Textbox(label="Accuracy Info")

#     state = gr.State()

#     submit_query_btn.click(
#         initial_query,
#         inputs=[query_input, state],
#         outputs=[confirmation_text, state]
#     )

#     Confirm_btn.click(
#         finalize_decision,
#         inputs=[query_input, confirm_use, selected_model, selected_dataset, state],
#         outputs=[result_text, accuracy_text, confirmation_text]
#     )

# app.launch()

    # confirm_database = gr.Checkbox(label="Confirm Database", value=False)
    # confirm_model = gr.Checkbox(label="Confirm Model Use", value=False)
    # state = gr.State()
    # confirm_text = gr.Textbox(label="Confirmation Needed", interactive=False)
    # output1 = gr.Textbox(lines=1, max_lines=10, label="Model Name or Query Method")
    # output2 = gr.Textbox(lines=1, max_lines=10, label="Query Result")
    # output3 = gr.Textbox(lines=1, max_lines=10, label="Prediction Accuracy Info")

    # dataset_dropdown = gr.Dropdown(label="Choose a dataset", choices=dataset_files, value=dataset_files[0])


    # with gr.Row():
    #     submit_btn = gr.Button("Submit",variant="primary")

    # submit_btn.click(
    #     fn=process_query,
    #     inputs=[
    #         query_input,       # query
    #         confirm_use,       # confirm_use
    #         selected_model,    # selected_model
    #         selected_dataset,  # selected_dataset
    #         use_new,
    #         gr.State()         # state
    #     ],
    #     outputs=[confirmation_text, model_name_text, result_text, accuracy_text, gr.State()]
        # inputs=[query_input, use_existing, dataset_dropdown, model_dropdown,  gr.State()],
        # inputs=[query_input, confirm_database, confirm_model, state],
        # outputs=[gr.Textbox(label="Model Name"), gr.Textbox(label="Result"), gr.Textbox(label="Accuracy Info"), gr.State(), gr.Textbox(label="Confirmation Needed")]
        # outputs=[output1, output2, output3, state, confirm_text]
#     )

# app.launch()









